{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbcba58",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchcodec\n",
    "!pip install soundfile\n",
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3e19c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffwave.inference import predict as diffwave_predict\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c474dff3",
   "metadata": {},
   "source": [
    "# Perform the following to get the mel spectrograms\n",
    "\n",
    "Check the preprocess file to see what mel spectrograms they use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62c80f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing: 100%|██████████████████████████████| 1/1 [00:02<00:00,  2.43s/it]\n"
     ]
    }
   ],
   "source": [
    "!python -m diffwave.preprocess ./audio_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "466bf824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[32m/home/wongbr55/diffwave_expose_latent_space/src/diffwave/inference.py\u001b[39m(\u001b[92m63\u001b[39m)\u001b[36mpredict\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[32m     61\u001b[39m     T = []\n",
      "\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;28;01min\u001b[39;00m range(len(inference_noise_schedule)):\n",
      "\u001b[32m---> 63\u001b[39m       \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;28;01min\u001b[39;00m range(len(training_noise_schedule) - \u001b[32m1\u001b[39m):\n",
      "\u001b[32m     64\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m talpha_cum[t+\u001b[32m1\u001b[39m] <= alpha_cum[s] <= talpha_cum[t]:\n",
      "\u001b[32m     65\u001b[39m           twiddle = (talpha_cum[t]**\u001b[32m0.5\u001b[39m - alpha_cum[s]**\u001b[32m0.5\u001b[39m) / (talpha_cum[t]**\u001b[32m0.5\u001b[39m - talpha_cum[t+\u001b[32m1\u001b[39m]**\u001b[32m0.5\u001b[39m)\n",
      "\n",
      "50\n",
      "50\n",
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "mel = np.load(\"./audio_wav/LJ001-0003.wav.spec.npy\")  # shape: (n_mels, time)\n",
    "\n",
    "# # Convert to PyTorch tensor\n",
    "mel = torch.from_numpy(mel).float()\n",
    "\n",
    "\n",
    "model_dir = './pretrained_model/diffwave-ljspeech-22kHz-1000578.pt'\n",
    "audio, sample_rate = diffwave_predict(mel, model_dir, fast_sampling=False)\n",
    "\n",
    "write(\"output.wav\", sample_rate, audio.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f16a367d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 213248])\n"
     ]
    }
   ],
   "source": [
    "print(audio.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd252dd7",
   "metadata": {},
   "source": [
    "## Steps to train to fit brain signals into diffusion pipeline\n",
    "1. Get mel spectrograms of all of the GT sentences\n",
    "2. Use those to get latent variables to train\n",
    "3. Train model(s) to give use mel spectrograms and latent variables\n",
    "\n",
    "### Notes on setup\n",
    "- Must choose whether we use fast sampling or not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffwave_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
